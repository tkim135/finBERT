>-*-*-*-*-*-*-<
LR: 5e-05, WD: 0.001, Seed: 42, BS: 4, Max Length: 60, Gradual Unfreeze: False, Discriminative Finetuning: False, Weight: /home/ubuntu/finBERT/weights/hf_ckpt_decay0.5_lr1.e-6_ss1024_bs16_results_finbert/pytorch_model_lr1.e-6_wd0.5_ckpt10.bin
>-*-*-*-*-*-*-<
Some weights of the model checkpoint at None were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at None and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at None were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at None and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at None were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at None and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at None were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at None and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]Downloading: 2.92kB [00:00, 1.70MB/s]                   
Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]Downloading: 2.92kB [00:00, 1.84MB/s]                   
Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]Downloading: 2.92kB [00:00, 3.35MB/s]                   
Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]Downloading: 2.92kB [00:00, 3.66MB/s]                   
  0%|          | 0/6 [00:00<?, ?it/s]
  0%|          | 0/239 [00:00<?, ?it/s][A  0%|          | 0/6 [00:00<?, ?it/s]Training on batches...
  0%|          | 0/6 [00:00<?, ?it/s]

  0%|          | 0/239 [00:00<?, ?it/s][A  0%|          | 0/239 [00:00<?, ?it/s][A  0%|          | 0/6 [00:00<?, ?it/s]
  0%|          | 0/239 [00:00<?, ?it/s][A  0%|          | 0/239 [00:01<?, ?it/s]
  0%|          | 0/6 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "example.py", line 602, in <module>
    results = main(lr=lr, wd=wd, seed=seed, name=args.name, weight=weight, bs=bs, max_length=max_length, gradual_unfreeze=gradual_unfreeze, discriminate=discriminate)
  File "example.py", line 522, in main
    train_labels, train_predict, train_loss = train(accelerator, model, train_dataloader, optimizer, scheduler, device, i, gradual_unfreeze)
  File "example.py", line 203, in train
    logits = outputs_loss['logits']
IndexError: too many indices for tensor of dimension 0
  0%|          | 0/239 [00:01<?, ?it/s]
  0%|          | 0/6 [00:01<?, ?it/s]
  0%|          | 0/239 [00:01<?, ?it/s]
  0%|          | 0/6 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "example.py", line 602, in <module>
    results = main(lr=lr, wd=wd, seed=seed, name=args.name, weight=weight, bs=bs, max_length=max_length, gradual_unfreeze=gradual_unfreeze, discriminate=discriminate)
  File "example.py", line 522, in main
    train_labels, train_predict, train_loss = train(accelerator, model, train_dataloader, optimizer, scheduler, device, i, gradual_unfreeze)
  File "example.py", line 203, in train
    logits = outputs_loss['logits']
IndexError: too many indices for tensor of dimension 0
Traceback (most recent call last):
  File "example.py", line 602, in <module>
    results = main(lr=lr, wd=wd, seed=seed, name=args.name, weight=weight, bs=bs, max_length=max_length, gradual_unfreeze=gradual_unfreeze, discriminate=discriminate)
  File "example.py", line 522, in main
    train_labels, train_predict, train_loss = train(accelerator, model, train_dataloader, optimizer, scheduler, device, i, gradual_unfreeze)
  File "example.py", line 203, in train
    logits = outputs_loss['logits']
IndexError: too many indices for tensor of dimension 0
  0%|          | 0/239 [00:01<?, ?it/s]
  0%|          | 0/6 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "example.py", line 602, in <module>
    results = main(lr=lr, wd=wd, seed=seed, name=args.name, weight=weight, bs=bs, max_length=max_length, gradual_unfreeze=gradual_unfreeze, discriminate=discriminate)
  File "example.py", line 522, in main
    train_labels, train_predict, train_loss = train(accelerator, model, train_dataloader, optimizer, scheduler, device, i, gradual_unfreeze)
  File "example.py", line 203, in train
    logits = outputs_loss['logits']
IndexError: too many indices for tensor of dimension 0
Traceback (most recent call last):
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', 'example.py', '--name', 'decay0.5_lr1e-6_ckpt10', '--weight', '/home/ubuntu/finBERT/weights/hf_ckpt_decay0.5_lr1.e-6_ss1024_bs16_results_finbert/pytorch_model_lr1.e-6_wd0.5_ckpt10.bin', '--lr', '5e-5', '--wd', '0.001', '--seed', '42', '--bs', '4', '--max_length', '60', '--gradual_unfreeze', 'False', '--discriminate', 'False']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 78921
Killing subprocess 78922
Killing subprocess 78923
Killing subprocess 78924
Traceback (most recent call last):
  File "/home/ubuntu/.local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/accelerate/commands/accelerate_cli.py", line 41, in main
    args.func(args)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/accelerate/commands/launch.py", line 307, in launch_command
    multi_gpu_launcher(args)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/accelerate/commands/launch.py", line 151, in multi_gpu_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'torch.distributed.launch', '--use_env', '--nproc_per_node', '4', 'example.py', '--name', 'decay0.5_lr1e-6_ckpt10', '--weight', '/home/ubuntu/finBERT/weights/hf_ckpt_decay0.5_lr1.e-6_ss1024_bs16_results_finbert/pytorch_model_lr1.e-6_wd0.5_ckpt10.bin', '--lr', '5e-5', '--wd', '0.001', '--seed', '42', '--bs', '4', '--max_length', '60', '--gradual_unfreeze', 'False', '--discriminate', 'False']' returned non-zero exit status 1.
