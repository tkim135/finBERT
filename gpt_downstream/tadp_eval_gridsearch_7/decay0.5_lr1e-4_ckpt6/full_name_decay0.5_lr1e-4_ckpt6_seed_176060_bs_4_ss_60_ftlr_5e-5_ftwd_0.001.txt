>-*-*-*-*-*-*-<
LR: 5e-05, WD: 0.001, Seed: 176060, BS: 4, Max Length: 60, Gradual Unfreeze: False, Discriminative Finetuning: False, Weight: /home/ubuntu/finBERT/weights/hf_ckpt_decay0.5_lr1.e-4_ss1024_bs16_results_finbert/pytorch_model_lr1.e-4_wd0.5_ckpt6.bin, Use Smaller Vocab: True
>-*-*-*-*-*-*-<
Traceback (most recent call last):
  File "example.py", line 617, in <module>
    results = main(lr=lr, wd=wd, seed=seed, name=args.name, weight=weight, bs=bs, max_length=max_length, gradual_unfreeze=gradual_unfreeze, discriminate=discriminate, use_smaller_vocab=use_smaller_vocab)
  File "example.py", line 413, in main
    model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=None, state_dict=checkpoint, config=model_config)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1356, in from_pretrained
    _fast_init=_fast_init,
  File "/home/ubuntu/.local/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1497, in _load_state_dict_into_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for GPT2ForSequenceClassification:
	size mismatch for transformer.wte.weight: copying a param with shape torch.Size([50257, 1600]) from checkpoint, the shape in current model is torch.Size([50260, 1600]).
Traceback (most recent call last):
  File "example.py", line 617, in <module>
    results = main(lr=lr, wd=wd, seed=seed, name=args.name, weight=weight, bs=bs, max_length=max_length, gradual_unfreeze=gradual_unfreeze, discriminate=discriminate, use_smaller_vocab=use_smaller_vocab)
  File "example.py", line 413, in main
    model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=None, state_dict=checkpoint, config=model_config)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1356, in from_pretrained
    _fast_init=_fast_init,
  File "/home/ubuntu/.local/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1497, in _load_state_dict_into_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for GPT2ForSequenceClassification:
	size mismatch for transformer.wte.weight: copying a param with shape torch.Size([50257, 1600]) from checkpoint, the shape in current model is torch.Size([50260, 1600]).
Traceback (most recent call last):
  File "example.py", line 617, in <module>
    results = main(lr=lr, wd=wd, seed=seed, name=args.name, weight=weight, bs=bs, max_length=max_length, gradual_unfreeze=gradual_unfreeze, discriminate=discriminate, use_smaller_vocab=use_smaller_vocab)
  File "example.py", line 413, in main
    model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=None, state_dict=checkpoint, config=model_config)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1356, in from_pretrained
    _fast_init=_fast_init,
  File "/home/ubuntu/.local/lib/python3.6/site-packages/transformers/modeling_utils.py", line 1497, in _load_state_dict_into_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for GPT2ForSequenceClassification:
	size mismatch for transformer.wte.weight: copying a param with shape torch.Size([50257, 1600]) from checkpoint, the shape in current model is torch.Size([50260, 1600]).
Traceback (most recent call last):
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', 'example.py', '--name', 'decay0.5_lr1e-4_ckpt6', '--weight', '/home/ubuntu/finBERT/weights/hf_ckpt_decay0.5_lr1.e-4_ss1024_bs16_results_finbert/pytorch_model_lr1.e-4_wd0.5_ckpt6.bin', '--lr', '5e-5', '--wd', '0.001', '--seed', '176060', '--bs', '4', '--max_length', '60', '--gradual_unfreeze', 'False', '--discriminate', 'False', '--use_smaller_vocab', 'True']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 2788
Killing subprocess 2789
Killing subprocess 2790
Killing subprocess 2792
Traceback (most recent call last):
  File "/home/ubuntu/.local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/accelerate/commands/accelerate_cli.py", line 41, in main
    args.func(args)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/accelerate/commands/launch.py", line 307, in launch_command
    multi_gpu_launcher(args)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/accelerate/commands/launch.py", line 151, in multi_gpu_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'torch.distributed.launch', '--use_env', '--nproc_per_node', '4', 'example.py', '--name', 'decay0.5_lr1e-4_ckpt6', '--weight', '/home/ubuntu/finBERT/weights/hf_ckpt_decay0.5_lr1.e-4_ss1024_bs16_results_finbert/pytorch_model_lr1.e-4_wd0.5_ckpt6.bin', '--lr', '5e-5', '--wd', '0.001', '--seed', '176060', '--bs', '4', '--max_length', '60', '--gradual_unfreeze', 'False', '--discriminate', 'False', '--use_smaller_vocab', 'True']' returned non-zero exit status 1.
